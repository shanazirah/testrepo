{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for Plain GAN and WGAN implementation. Shamelessly adapt from here : https://github.com/eriklindernoren/PyTorch-GAN/\n",
    "\n",
    "You can adapt this for demo on your presentation -- of course there will be always bonus points if Python notebook is included ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking whether everything is working (including installing PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your current folder\n",
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing torch if you do not have it yet. Should work also for Windows\n",
    "! pip3 install torch torchvision\n",
    "! mkdir datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if everything is installed nicely\n",
    "! python -c \"import torch; print('torch version: ', torch.__version__)\"\n",
    "! python -c \"import torchvision; print('torchvision version: ', torchvision.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just for creating the model + dataset + figures folder \n",
    "\n",
    "! mkdir -p gan\n",
    "! mkdir -p gan/model/\n",
    "! mkdir -p gan/fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We import all the neccessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils import data\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Load dataset (which is MNIST for our case here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mnist_trainset = datasets.MNIST(\n",
    "    root='./datasets/mnist', \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transforms.Compose([transforms.Resize(28), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize([0.5], [0.5])])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Infer from the output below the size of the dataset. Do you know why we have one image following this shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist_trainset.targets.shape, mnist_trainset.data.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "dataloader = data.DataLoader(mnist_trainset, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We check a batch of our samples (which size should it have?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, targets = next(iter(dataloader))\n",
    "print( imgs.mean(), imgs.std(), imgs.max(), imgs.min()  )\n",
    "print( imgs.dtype, targets.dtype )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Discriminator and Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # why taking 784 as the first dimension? \n",
    "        model = [nn.Linear(784, 256), \n",
    "                 nn.LeakyReLU(0.2, inplace=True),\n",
    "                 nn.Linear(256, 128), \n",
    "                 nn.LeakyReLU(0.2, inplace=True),\n",
    "                 nn.Linear(128, 1),\n",
    "                 nn.Sigmoid()\n",
    "                ]\n",
    "        self.model = nn.Sequential(*model) \n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=100):\n",
    "        super(Generator, self).__init__()\n",
    "        model = [nn.Linear(latent_dim, 128), \n",
    "                 nn.BatchNorm1d(128, 0.8),\n",
    "                 nn.LeakyReLU(0.2, inplace=True),\n",
    "                 nn.Linear(128, 256), \n",
    "                 nn.BatchNorm1d(256, 0.8),\n",
    "                 nn.LeakyReLU(0.2, inplace=True),\n",
    "                 nn.Linear(256, 512),\n",
    "                 nn.BatchNorm1d(512, 0.8),\n",
    "                 nn.LeakyReLU(0.2, inplace=True),\n",
    "                 nn.Linear(512, 1024),\n",
    "                 nn.BatchNorm1d(1024, 0.8),\n",
    "                 nn.LeakyReLU(0.2, inplace=True),\n",
    "                 nn.Linear(1024, 784),\n",
    "                 nn.Tanh()\n",
    "                ]\n",
    "        self.model = nn.Sequential(*model)\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), 1, 28, 28)\n",
    "        return img \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is for loading model into either GPU or CPU -- depending if your machine has GPU or not \n",
    "\n",
    "use_cuda = torch.cuda.is_available() \n",
    "latent_dim = 100  # so which size of the noise we have here?\n",
    "if use_cuda:\n",
    "    G = Generator(latent_dim=latent_dim).cuda()\n",
    "    D = Discriminator().cuda()\n",
    "else:\n",
    "    G = Generator(latent_dim=latent_dim)\n",
    "    D = Discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do you still remember why we use `torch.nn.BCELoss` as a loss?\n",
    "- Adam is one of (if not) the most popular stochastic GD optimization scheme for neural network. Read more [here](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) for the PyTorch documentation and https://arxiv.org/abs/1412.6980 the link for the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again the first line is just to ensure we have nice behaviour when playing in both GPU and CPU\n",
    "Tensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "\n",
    "# Some visualization functions\n",
    "\n",
    "def show_img(img): # img: torch.Size([28, 28])\n",
    "    #print(img.shape)\n",
    "    data = img.squeeze(0).cpu().detach().numpy()\n",
    "    #print(data.shape)\n",
    "    plt.axis('off')\n",
    "    imgplot = plt.imshow(data, cmap=\"gray\")\n",
    "    \n",
    "def show_imgs(imgs, n_height=8, n_width=8):  # imgs: torch.Size([64, 1, 28, 28])\n",
    "    assert len(imgs) == n_height* n_width\n",
    "    imgs = imgs.squeeze(1).cpu().detach().numpy()\n",
    "    fig = plt.figure(figsize = (n_height, n_width) )\n",
    "    gs1 = gridspec.GridSpec(n_height, n_width)\n",
    "    gs1.update(wspace=0.025, hspace=0.05) # set the spacing between axes.\n",
    "    for i in range(len(imgs)):\n",
    "        plottable_image = imgs[i]\n",
    "        # i = i + 1 # grid spec indexes from 0\n",
    "        ax = plt.subplot(gs1[i])\n",
    "        ax.axis('off')\n",
    "        ax.set_aspect('equal')\n",
    "        ax.imshow(plottable_image, cmap='gray')\n",
    "\n",
    "def sample_img(G, data_loader, PATH=\"test.jpg\"):\n",
    "    G.eval()\n",
    "    imgs, targets = next(iter(data_loader))\n",
    "    imgs, targets = imgs.type(Tensor), targets.type(Tensor)\n",
    "    batch_size = imgs.size(0)\n",
    "    z = torch.zeros((batch_size,latent_dim)).normal_(0, 1).type(Tensor)\n",
    "    gen_imgs_batch = G(z)\n",
    "    show_imgs(gen_imgs_batch)\n",
    "    plt.savefig(PATH)\n",
    "    plt.show()\n",
    "    G.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a batch of sample images\n",
    "imgs, targets = next(iter(dataloader))\n",
    "show_imgs(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the plain GAN -- warning: to finish 200 epochs without GPU it can take a very long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "model_name = \"gan\"\n",
    "start_epo = 0\n",
    "n_epochs = 200  # playing with this for some fun\n",
    "print_freq = 10 # this is for outputting the generator's images + printing the log\n",
    "\n",
    "# Loss function\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "# Optimizers, betas are exponential decay rates for the moment estimate (read more on the links above)\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recheck the architecture of G and D\n",
    "print(G.train())\n",
    "print(D.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "        # Configure input\n",
    "        batch_size = imgs.shape[0]\n",
    "        real_imgs = imgs.type(Tensor)\n",
    "        # Sample noise as generator input\n",
    "        z = torch.randn(batch_size, latent_dim).type(Tensor)\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = G(z)\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones((batch_size, 1)).type(Tensor)\n",
    "        fake = torch.zeros((batch_size, 1)).type(Tensor)\n",
    "        if (epoch+i)%2==0:\n",
    "            # g_loss: Loss measures generator's ability to fool the discriminator\n",
    "            g_loss = loss_fn(D(gen_imgs), valid)\n",
    "            # g_loss: loss backward\n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss.backward(retain_graph=True)\n",
    "            optimizer_G.step()\n",
    "        else:\n",
    "            # d_loss: Measure discriminator's ability to classify real from generated samples\n",
    "            real_loss = loss_fn(D(real_imgs), valid)\n",
    "            fake_loss = loss_fn(D(gen_imgs.detach()), fake)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            # d_loss: loss backward        \n",
    "            optimizer_D.zero_grad()\n",
    "            d_loss.backward(retain_graph=True)\n",
    "            optimizer_D.step()\n",
    "\n",
    "    if epoch==0 or epoch % print_freq == (print_freq-1) or epoch==n_epochs-1:         \n",
    "        print(\n",
    "            f\"Epoch {epoch + 1} / {n_epochs} | Discriminator loss: {d_loss.item()} | Generator loss: {g_loss.item()}\"\n",
    "        )\n",
    "        # save out\n",
    "        this_epo_str = str(epoch+start_epo).zfill(4) \n",
    "        torch.save(G.state_dict(), f\"{model_name}/model/G_{this_epo_str}\")\n",
    "        torch.save(D.state_dict(), f\"{model_name}/model/D_{this_epo_str}\")\n",
    "        # sampling using the generator at checked-point epoch\n",
    "        sample_img(G, dataloader, f\"{model_name}/fig/{this_epo_str}.jpg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After training is finished, we have a Generator to play with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img(G, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same thing, but a bit different\n",
    "! mkdir -p wgan/model/\n",
    "! mkdir -p wgan/fig/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that now the architecture is almost (99%) the same with plain GAN above\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        model = [nn.Linear(784, 256), \n",
    "                 nn.LeakyReLU(0.2, inplace=True),\n",
    "                 nn.Linear(256, 128), \n",
    "                 nn.LeakyReLU(0.2, inplace=True),\n",
    "                 nn.Linear(128, 1),\n",
    "                 #nn.Sigmoid()  # for wGAN\n",
    "                ]\n",
    "        self.model = nn.Sequential(*model) \n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=100):\n",
    "        super(Generator, self).__init__()\n",
    "        model = [nn.Linear(latent_dim, 128), \n",
    "                 nn.BatchNorm1d(128, 0.8),\n",
    "                 nn.LeakyReLU(0.2, inplace=True),\n",
    "                 nn.Linear(128, 256), \n",
    "                 nn.BatchNorm1d(256, 0.8),\n",
    "                 nn.LeakyReLU(0.2, inplace=True),\n",
    "                 nn.Linear(256, 512),\n",
    "                 nn.BatchNorm1d(512, 0.8),\n",
    "                 nn.LeakyReLU(0.2, inplace=True),\n",
    "                 nn.Linear(512, 1024),\n",
    "                 nn.BatchNorm1d(1024, 0.8),\n",
    "                 nn.LeakyReLU(0.2, inplace=True),\n",
    "                 nn.Linear(1024, 784),\n",
    "                 nn.Tanh()\n",
    "                ]\n",
    "        self.model = nn.Sequential(*model)\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), 1, 28, 28)\n",
    "        return img \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And also different optimizers \n",
    "optimizer_G = torch.optim.RMSprop(G.parameters(), lr=0.00005)\n",
    "optimizer_D = torch.optim.RMSprop(D.parameters(), lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "model_name = \"wgan\"\n",
    "start_epo = 0\n",
    "n_epochs = 200\n",
    "print_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recheck the architecture of G and D\n",
    "print(G.train())\n",
    "print(D.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training -- can you spot the very important differences here compared to plain GANs above?\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "        batch_size = imgs.size(0)\n",
    "        # Configure input\n",
    "        real_imgs = imgs.type(Tensor)\n",
    "        # Sample noise as generator input\n",
    "        z = torch.randn(batch_size, latent_dim).type(Tensor)\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = G(z)\n",
    "\n",
    "        if (epoch + i) % 2 == 0:\n",
    "            # g_loss: Loss measures generator's ability to fool the discriminator\n",
    "            g_loss = -torch.mean(D(gen_imgs))\n",
    "            # g_loss: loss backward\n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss.backward(retain_graph=True)\n",
    "            optimizer_G.step()\n",
    "        else:\n",
    "            # d_loss: Measure discriminator's ability to classify real from generated samples\n",
    "            d_loss = torch.mean(D(gen_imgs.detach())) - torch.mean(D(real_imgs))\n",
    "            # d_loss: loss backward        \n",
    "            optimizer_D.zero_grad()\n",
    "            d_loss.backward(retain_graph=True)\n",
    "            optimizer_D.step()\n",
    "            \n",
    "    if epoch == 0 or epoch % print_freq == (print_freq-1) or epoch == n_epochs-1:         \n",
    "        print(\n",
    "            f\"Epoch {epoch + 1} / {n_epochs} | Discriminator loss: {d_loss.item()} | Generator loss: {g_loss.item()}\"\n",
    "        )\n",
    "        # save out\n",
    "        this_epo_str = str(epoch + start_epo).zfill(4) \n",
    "        torch.save(G.state_dict(), f\"{model_name}/model/G_{this_epo_str}\")\n",
    "        torch.save(D.state_dict(), f\"{model_name}/model/D_{this_epo_str}\")\n",
    "        # sampling using the generator at checked-point epoch\n",
    "        sample_img(G, dataloader, f\"{model_name}/fig/{this_epo_str}.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img(G, dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
